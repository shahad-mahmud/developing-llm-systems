{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Configs:\n",
    "    GPT_MODEL = \"gpt-4o\"\n",
    "\n",
    "settings = Configs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=settings.GPT_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face's Transformers library offers several options for positional embeddings in the BERT model. As of my last update, these positional embedding options include:\n",
      "\n",
      "1. **Learned Positional Embeddings**: This is the standard method used in the original BERT model. It involves learning a fixed set of positional embeddings during training, which are then added to the token embeddings.\n",
      "\n",
      "2. **Sinusoidal Positional Embeddings**: This method, used by models like Transformer-XL and GPT, involves using a fixed set of sinusoidal functions to generate positional embeddings. This method does not require learning additional parameters for positions.\n",
      "\n",
      "To configure the positional embeddings in a BERT model using the Transformers library, you typically need to define the embeddings in the model configuration. The default BERT model configuration uses learned positional embeddings.\n",
      "\n",
      "Here's a simplified example to define and use these positional embeddings with the BERT model:\n",
      "\n",
      "### Using Learned Positional Embeddings (default in BERT):\n",
      "```python\n",
      "from transformers import BertModel, BertConfig\n",
      "\n",
      "# Define the configuration for BERT\n",
      "config = BertConfig()\n",
      "\n",
      "# Initialize a BERT model using this configuration\n",
      "model = BertModel(config)\n",
      "\n",
      "# The model will use learned positional embeddings by default\n",
      "```\n",
      "\n",
      "### Using Sinusoidal Positional Embeddings:\n",
      "To use sinusoidal positional embeddings, you may need to modify the model code or use a different model that supports this type of embedding natively, such as Transformer-XL or GPT. However, if you strictly want to use BERT with sinusoidal embeddings, it would typically require custom implementation, as BERT does not support this out-of-the-box.\n",
      "\n",
      "Below is a conceptual example (note that this is illustrative and may require further adjustment to work with the actual Transformers library):\n",
      "\n",
      "```python\n",
      "from transformers import BertModel, BertConfig, PreTrainedModel\n",
      "import torch\n",
      "import math\n",
      "\n",
      "class SinusoidalPositionalEmbedding(torch.nn.Module):\n",
      "    def __init__(self, embedding_dim, max_len=512):\n",
      "        super().__init__()\n",
      "        self.embedding_dim = embedding_dim\n",
      "        \n",
      "        # Create a long enough 'P'\n",
      "        position = torch.arange(0, max_len).unsqueeze(1)\n",
      "        div_term = torch.exp(torch.arange(0, embedding_dim, 2) * -(math.log(10000.0) / embedding_dim))\n",
      "        pe = torch.zeros(max_len, embedding_dim)\n",
      "        pe[:, 0::2] = torch.sin(position * div_term)\n",
      "        pe[:, 1::2] = torch.cos(position * div_term)\n",
      "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
      "        self.register_buffer('pe', pe)\n",
      "\n",
      "    def forward(self, x):\n",
      "        return self.pe[:x.size(0), :]\n",
      "\n",
      "class BertModelWithSinusoidalPositionalEmbedding(PreTrainedModel):\n",
      "    def __init__(self, config):\n",
      "        super().__init__(config)\n",
      "        self.bert = BertModel(config)\n",
      "        self.pos_emb = SinusoidalPositionalEmbedding(config.hidden_size)\n",
      "\n",
      "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None):\n",
      "        embeddings = self.bert.embeddings.word_embeddings(input_ids)\n",
      "        \n",
      "        if position_ids is None:\n",
      "            position_ids = torch.arange(embeddings.size(1), dtype=torch.long, device=embeddings.device)\n",
      "            position_ids = position_ids.unsqueeze(0).expand(embeddings.size(0), embeddings.size(1))\n",
      "        \n",
      "        embeddings += self.pos_emb(position_ids)\n",
      "        # Continue with the forward pass using modified embeddings\n",
      "        return self.bert(inputs_embeds=embeddings, attention_mask=attention_mask, token_type_ids=token_type_ids, head_mask=head_mask)\n",
      "\n",
      "# Define the configuration for BERT\n",
      "config = BertConfig()\n",
      "\n",
      "# Initialize your custom model\n",
      "model = BertModelWithSinusoidalPositionalEmbedding(config)\n",
      "```\n",
      "\n",
      "### Note:\n",
      "The actual implementation might vary depending on updates to the library and specific model requirements. Always refer to the latest version of the Hugging Face Transformers documentation and source code for the most accurate and up-to-date information.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Give me the list of positional embedding options for Bert model using Hugging Face's Transformers library?\"\n",
    "response = llm.invoke(prompt)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "temp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
