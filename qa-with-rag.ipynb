{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in this notebook corresponds to [this blog post](https://shahadmahmud.com/en/ml/llm/retriever-augmented-generation-rag-with-llms). You can go through the blog post to understand the code in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import UnstructuredHTMLLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Configs:\n",
    "    GPT_MODEL = \"gpt-4o\"\n",
    "\n",
    "    SOURCE_FILE = \"data/example/BERT.html\"\n",
    "\n",
    "settings = Configs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the knowledge base (KB)\n",
    "\n",
    "We would use the [BERT documentation](https://huggingface.co/docs/transformers/en/model_doc/bert) from Hugging Face as the KB for this task. We have pre-downloaded the webpage and is available in the `data/example/BERT.html` file. We would use the `unstructured` library to parse the HTML file and extract the text content from the webpage.\n",
    "\n",
    "If you want to load the webpage from the internet, look at [this documentation](https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/html/) from Langchain.\n",
    "\n",
    "### Loading the webpage\n",
    "\n",
    "We would load the webpage and extract the text content from the webpage using the `UnstructuredHTMLLoader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = UnstructuredHTMLLoader(settings.SOURCE_FILE)\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the text content\n",
    "\n",
    "Loading the entire text content from the webpage would be too much for the model to handle. We can even hit the context length limitation. Also, the LLM may not get the right piece information from the whole text. \n",
    "\n",
    "Thats why we would split the text content into smaller and manageable chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=32)\n",
    "chunks = splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Vector Database\n",
    "\n",
    "We would create a vector database from the text chunks. We would use the `OpenAIEmbeddings` to encode the text chunks into vectors and the `Faiss` library to create the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "db = FAISS.from_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing the Retrieval Pipeline\n",
    "\n",
    "We would develop a retrieval pipeline that would take a query and return the most relevant text chunks from the knowledge base. We will combine the texts and pass it to the LLM model along the original Query to generate the answer.\n",
    "\n",
    "For the pipeline, we will use a prompt template and chain consisting of retriever, LLM and output parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"You are a helpful AI assistant that can answer questions from given contexts.\n",
    "\n",
    "The given contexts are:\n",
    "{contexts}\n",
    "\n",
    "The question is:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatOpenAI(model=settings.GPT_MODEL, temperature=0.3)\n",
    "retriever = db.as_retriever(k=10)\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"contexts\": itemgetter(\"question\")\n",
    "        | retriever\n",
    "        | (lambda x: [d.page_content for d in x]),\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To use Scaled Dot Product Attention (SDPA) with BERT in the transformers library, you can follow these steps:\n",
       "\n",
       "1. **Load the BERT Model with SDPA**: You need to load the BERT model with the `attn_implementation` parameter set to `\"sdpa\"`. This can be done using the `BertModel.from_pretrained` method.\n",
       "\n",
       "2. **Use Half-Precision for Speedups**: For optimal performance, it is recommended to load the model in half-precision (e.g., `torch.float16` or `torch.bfloat16`).\n",
       "\n",
       "Here is an example code snippet to illustrate this:\n",
       "\n",
       "```python\n",
       "from transformers import BertModel\n",
       "\n",
       "# Load the BERT model with SDPA and in half-precision\n",
       "model = BertModel.from_pretrained(\n",
       "    \"bert-base-uncased\", \n",
       "    torch_dtype=torch.float16, \n",
       "    attn_implementation=\"sdpa\"\n",
       ")\n",
       "\n",
       "# Now the model is ready to be used for training or inference\n",
       "```\n",
       "\n",
       "### Additional Tips:\n",
       "- **Padding**: Since BERT uses absolute position embeddings, it is usually advised to pad the inputs on the right rather than the left.\n",
       "- **Training and Inference**: You can use this model for tasks like masked language modeling (MLM) and next sentence prediction (NSP), which are the objectives BERT was trained on.\n",
       "\n",
       "### Example Usage for Inference:\n",
       "Hereâ€™s an example of how you might use the model for a question-answering task:\n",
       "\n",
       "```python\n",
       "from transformers import BertTokenizer\n",
       "\n",
       "# Initialize the tokenizer\n",
       "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
       "\n",
       "# Define the question and text\n",
       "question = \"Who was Jim Henson?\"\n",
       "text = \"Jim Henson was a nice puppet\"\n",
       "\n",
       "# Tokenize the inputs\n",
       "inputs = tokenizer(question, text, return_tensors=\"pt\")\n",
       "\n",
       "# Perform inference\n",
       "outputs = model(**inputs)\n",
       "\n",
       "# Extract start and end scores for the answer span\n",
       "start_scores = outputs.start_logits\n",
       "end_scores = outputs.end_logits\n",
       "\n",
       "# Process the scores to get the final answer (not shown here)\n",
       "```\n",
       "\n",
       "By following these steps, you can effectively use SDPA with BERT in the transformers library."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = chain.invoke({\n",
    "    \"question\": \"How can I use SDPA with BERT in transformers?\"\n",
    "})\n",
    "\n",
    "display(Markdown(response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "temp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
